---
title: "DSCI 310: Airbnb Price Analysis"
author: "Oliver Gullery, Prithvi, Riddhi Battu, Rashi"
format: 
    html: 
        toc: true
        toc-depth: 3
    pdf:
        toc: true
        toc-depth: 3
# bibliography: references.bibit
editor: source
jupyter: 
    kernel: dsci 

---


```{python}
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import plotly.express as px
import plotly.graph_objects as go
import folium
```

## Aim
The aim of this data analysis project is to identify which of the factors in the Airbnb Kaggle Dataset are strong predictors of price. Doing so will allow us obtain information which can determine if Airbnbs are accurately priced. 


```{python}
# loading the data
data = pd.read_csv("../data/raw/AB_NYC_2019.csv")
data.head(3)
```


### Data Shape & Datatypes
```{python}
print(f'Data Shape: {data.shape}\n')



print(f'Data datatypes: \n{data.dtypes}')
```

### Summary Statistics
```{python}
data.info()

data.describe()

```

### Identifying Null values & Duplicates
```{python}

print(f'Null Values: {data.isna().sum()}\n')

print(f'Duplicated Values: {data.duplicated().sum()}')

```

### Correlation Matrix (Ranked with Top Ten Correlations)
```{python}

# Filter the DataFrame to include only numeric (int or float) columns
numeric_data = data.select_dtypes(include=[np.number])

# Creating correlation matrix from the filtered numeric DataFrame
corr_matrix = numeric_data.corr()

flattened_matrix = corr_matrix.stack().reset_index()
flattened_matrix.columns = ['Variable_1', 'Variable_2', 'Correlation']
flattened_matrix = flattened_matrix.loc[flattened_matrix['Variable_1'] != flattened_matrix['Variable_2']]
corr_column = flattened_matrix['Correlation']
flattened_matrix = flattened_matrix.iloc[abs(corr_column).argsort()[::-1]]

# Removing duplicated correlations - keeping the first instance (highest absolute value)
flattened_matrix = flattened_matrix.loc[~flattened_matrix['Correlation'].duplicated(keep='first')]

print(f'Top 10 Variable Correlations: \n{flattened_matrix.head(10)}')

```

## Takwaways From Preliminary EDA
- Our shape function tells us we have 48895 rows and 16 features which includes our target variable `price`. 

- The describe() function provided key summary statistics for our numerical columns which included the following metrics: count, mean, standard deviation, minimum, and maximum. This helps us obtain an idea as to the spread of our data. Our info() function gave us further information about the datatypes, columns, and amount of data we have. Through this analysis in addition to our dtypes() function we can identify that last_reviw is an object dtype but could be converted into a pandas datatime format to further utilize pandas datatime capabilities. Examples of such would be splitting the data into year, month and day to identify if there are any temporal patterns across months of years. 

- Our isna() function informed us of the null values which are included in the dataset. The columns `name` and `host_name` have 16 and 21 null values respectively, we can address this null values by imputing a blank string to indicate the information is not provided. Our `reviews_per_month` column and `last_review` column have 10052. As these have identical amounts of null values, we can assume that if a Airbnb listing doesn't have a review, instead zero the dataset put a null values. This can be addressed through imputing zero into the null vaues in the reviews_per_month column.  

### Summary
In order to prepare our data, for further analysis we must perform some preliminary feature engineering which will involve:

1. **Convert the data type of the `last_review` column to datetime.**
```{python}
data['last_review'] = pd.to_datetime(data['last_review'])
```
2. **Split the `last_review` column into year, month, and day columns.**
```{python}
data['year'] = data['last_review'].dt.year
data['month'] = data['last_review'].dt.month
data['day'] = data['last_review'].dt.day
```
3. **Impute zeros into the `reviews_per_month` column for null values.**
```{python}
data['reviews_per_month'] = data['reviews_per_month'].fillna(0)
```
4. **Impute an empty string into the `name` and `host_name` columns for null values.**
```{python}
data['name'] = data['name'].fillna('')
data['host_name'] = data['host_name'].fillna('')
```

After these changes are made we can proceed to split our data and perform further analysis.

## Prelimenary Feature Engineering
For the preliminary feature engineering, we will perform the steps above:

### Putting It All Together
Now, let's put all these steps together to perform your preliminary feature engineering tasks.

```{python}

# Convert last_review to datetime
data['last_review'] = pd.to_datetime(data['last_review'])

# Extract year, month, and day from last_review
data['year'] = data['last_review'].dt.year
data['month'] = data['last_review'].dt.month
data['day'] = data['last_review'].dt.day

# Impute zeros for reviews_per_month null values
data['reviews_per_month'] = data['reviews_per_month'].fillna(0)

# Impute empty string for name and host_name null values
data['name'] = data['name'].fillna('')
data['host_name'] = data['host_name'].fillna('')

data.head(3)
```