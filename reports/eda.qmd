---
title: "DSCI 310: Airbnb Price Analysis"
author: "Oliver Gullery, Prithvi, Riddhi, Rashi"
format: 
    html: 
        toc: true
        toc-depth: 3
    pdf:
        toc: true
        toc-depth: 3
# bibliography: references.bibit
editor: source
jupyter: python3
---


```{python}
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
```

## Aim
The aim of this data analysis project is to identify which of the factors in the Airbnb Kaggle Dataset are strong predictors of price. Doing so will allow us obtain information which can determine if Airbnbs are accurately priced. 


```{python}
# loading the data
data = pd.read_csv("../data/raw/AB_NYC_2019.csv")
data.head(3)
```


### Data Shape & Datatypes
```{python}
print(f'Data Shape: {data.shape}\n')



print(f'Data datatypes: \n{data.dtypes}')
```

### Summary Statistics
```{python}
data.info()

data.describe()

```

### Identifying Null values & Duplicates
```{python}

print(f'Null Values: {data.isna().sum()}\n')

print(f'Duplicated Values: {data.duplicated().sum()}')

```

### Correlation Matrix (Ranked with Top Ten Correlations)
```{python}

# creating correlation matrix
corr_matrix = data.corr()

# flattening matrix
flattened_matrix = corr_matrix.stack().reset_index()

#renaming columns
flattened_matrix.columns = ['Variable_1', 'Variable_2', 'Correlation']

# removing duplicate variable names
flattened_matrix = flattened_matrix.loc[flattened_matrix['Variable_1'] != flattened_matrix['Variable_2']]

corr_column = flattened_matrix['Correlation']

flattened_matrix = flattened_matrix.iloc[abs(corr_column).argsort()[::-1]]

flattened_matrix = flattened_matrix.loc[flattened_matrix['Correlation'].duplicated()]


print(f'Top 10 Variable Correlations: \n{flattened_matrix.head(10)}')

```


## Takwaways From Preliminary EDA
- Our shape function tells us we have 48895 rows and 16 features which includes our target variable `price`. 

- The describe() function provided key summary statistics for our numerical columns which included the following metrics: count, mean, standard deviation, minimum, and maximum. This helps us obtain an idea as to the spread of our data. Our info() function gave us further information about the datatypes, columns, and amount of data we have. Through this analysis in addition to our dtypes() function we can identify that last_reviw is an object dtype but could be converted into a pandas datatime format to further utilize pandas datatime capabilities. 

- Our isna() function informed us of the null values which are included in the dataset. The columns `name` and `host_name` have 16 and 21 null values respectively, we can address this null values by imputing a blank string to indicate the information is not provided. Our `reviews_per_month` column and `last_review` column have 10052. As these have identical amounts of null values, we can assume that if a Airbnb listing doesn't have a review, instead zero the dataset put a null values. This can be addressed through imputing zero into the null vaues in the reviews_per_month column.  

### Summary
In order to prepare our data, for further analysis we must perform some preliminary feature engineering which will involve:
- Converting the data type of the `last_review` column.
- Imputing zeros into the `reviews_per_month` null values
- Imputing an empty string into the null values for `name` and `host_name


## Prelimniary Feature Engineering