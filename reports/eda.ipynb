{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'DSCI 310: Airbnb Price Analysis'\n",
        "author: 'Oliver Gullery, Prithvi, Riddhi Battu, Rashi'\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    toc-depth: 3\n",
        "  pdf:\n",
        "    toc: true\n",
        "    toc-depth: 3\n",
        "editor: source\n",
        "---"
      ],
      "id": "2b87031a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import folium\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import xgboost as xg\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer"
      ],
      "id": "c5bbf879",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aim\n",
        "The aim of this data analysis project is to identify which of the factors in the Airbnb Kaggle Dataset are strong predictors of price. Doing so will allow us obtain information which can determine if Airbnbs are accurately priced. \n"
      ],
      "id": "4f054928"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# loading the data\n",
        "data = pd.read_csv(\"../data/raw/AB_NYC_2019.csv\")\n",
        "data.head(3)"
      ],
      "id": "c9872d1a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Shape & Datatypes\n"
      ],
      "id": "6056530e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f'Data Shape: {data.shape}\\n')\n",
        "\n",
        "\n",
        "\n",
        "print(f'Data datatypes: \\n{data.dtypes}')"
      ],
      "id": "28fa44ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary Statistics\n"
      ],
      "id": "c635130d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data.info()\n",
        "\n",
        "data.describe()"
      ],
      "id": "29c71b49",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Identifying Null values & Duplicates\n"
      ],
      "id": "2bfaf90e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f'Null Values: {data.isna().sum()}\\n')\n",
        "\n",
        "print(f'Duplicated Values: {data.duplicated().sum()}')"
      ],
      "id": "9181f33d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlation Matrix (Ranked with Top Ten Correlations)\n"
      ],
      "id": "5e979850"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# creating correlation matrix\n",
        "corr_matrix = data.select_dtypes('int64')\n",
        "\n",
        "def rank_correlations(corr_matrix):\n",
        "    # flattening matrix\n",
        "    flattened_matrix = corr_matrix.stack().reset_index()\n",
        "\n",
        "    #renaming columns\n",
        "    flattened_matrix.columns = ['Variable_1', 'Variable_2', 'Correlation']\n",
        "\n",
        "    # removing duplicate variable names\n",
        "    flattened_matrix = flattened_matrix.loc[flattened_matrix['Variable_1'] != flattened_matrix['Variable_2']]\n",
        "\n",
        "    corr_column = flattened_matrix['Correlation']\n",
        "\n",
        "    flattened_matrix = flattened_matrix.iloc[abs(corr_column).argsort()[::-1]]\n",
        "\n",
        "    flattened_matrix = flattened_matrix.loc[flattened_matrix['Correlation'].duplicated()]\n",
        "\n",
        "\n",
        "    print(f'Top 10 Variable Correlations: \\n{flattened_matrix.head(10)}')\n",
        "\n",
        "rank_correlations(corr_matrix)"
      ],
      "id": "3486826b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takwaways From Preliminary EDA\n",
        "- Our shape function tells us we have 48895 rows and 16 features which includes our target variable `price`. \n",
        "\n",
        "-Looking at the columns from our info() function we can identify that name is a text data which could provide some valuable insights. We can also infer that any id information (`id` and `host_id`) and variables such as `host_name` will not provide any key information, thus, we can drop them for our data analysis.\n",
        "\n",
        "- The describe() function provided key summary statistics for our numerical columns which included the following metrics: count, mean, standard deviation, minimum, and maximum. This helps us obtain an idea as to the spread of our data. Our info() function gave us further information about the datatypes, columns, and amount of data we have. Through this analysis in addition to our dtypes() function we can identify that last_reviw is an object dtype but could be converted into a pandas datatime format to further utilize pandas datatime capabilities. Examples of such would be splitting the data into year, month and day to identify if there are any temporal patterns across months of years. \n",
        "\n",
        "- Our isna() function informed us of the null values which are included in the dataset. The columns `name` and `host_name` have 16 and 21 null values respectively, we can address this null values by imputing a blank string to indicate the information is not provided. Our `reviews_per_month` column and `last_review` column have 10052. As these have identical amounts of null values, we can assume that if a Airbnb listing doesn't have a review, instead zero the dataset put a null values. This can be addressed through imputing zero into the null vaues in the reviews_per_month column.  \n",
        "\n",
        "### Summary\n",
        "In order to prepare our data for further analysis we must perform some preliminary feature engineering which will involve:\n",
        "- Convert `id` and `host_id` into object datatypes to prepare them to be dropped.\n",
        "- Converting the data type of the `last_review` column.\n",
        "    - Then split the `last_review` into year, month and day.\n",
        "- Imputing zeros into the `reviews_per_month` null values\n",
        "- Imputing an empty string into the null values for `name` and `host_name\n",
        "\n",
        "\n",
        "After these changes are made we can proceed to split our data into a train and test set and label our data by data type (numeric, categorical, text, and drop columns).\n",
        "### Preliminary Feature Engineering\n",
        "In order to prepare our data, for further analysis we must perform some preliminary feature engineering which will involve:\n",
        "\n",
        "1. **Convert `id` and `host_id` into object datatypes to prepare them to be dropped.**\n"
      ],
      "id": "88782538"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data['id'] = str(data['id'])\n",
        "data['host_id'] = str(data['host_id'])"
      ],
      "id": "5554d0b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. **Convert the data type of the `last_review` column to datetime.**\n"
      ],
      "id": "fafc3203"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data['last_review'] = pd.to_datetime(data['last_review'])"
      ],
      "id": "4892e4f8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. **Split the `last_review` column into year, month, and day columns.**\n"
      ],
      "id": "5f3675ba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data['year'] = data['last_review'].dt.year\n",
        "data['month'] = data['last_review'].dt.month\n",
        "data['day'] = data['last_review'].dt.day"
      ],
      "id": "99a4679b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. **Impute zeros into the `reviews_per_month` column for null values.**\n"
      ],
      "id": "76a3cbba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data['reviews_per_month'] = data['reviews_per_month'].fillna(0)"
      ],
      "id": "84696378",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. **Impute an empty string into the `name` and `host_name` columns for null values.**\n"
      ],
      "id": "2e5aaffb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data['name'] = data['name'].fillna('')\n",
        "data['host_name'] = data['host_name'].fillna('')"
      ],
      "id": "988b0823",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After these changes are made we can proceed to split our data and perform further analysis.\n",
        "\n",
        "## Prelimenary Feature Engineering\n",
        "For the preliminary feature engineering, we will perform the steps above:\n",
        "\n",
        "### Putting It All Together\n",
        "Now, let's put all these steps together to perform your preliminary feature engineering tasks.\n"
      ],
      "id": "aac4c22b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert last_review to datetime\n",
        "data['last_review'] = pd.to_datetime(data['last_review'])\n",
        "\n",
        "# Extract year, month, and day from last_review\n",
        "data['year'] = data['last_review'].dt.year\n",
        "data['month'] = data['last_review'].dt.month\n",
        "data['day'] = data['last_review'].dt.day\n",
        "\n",
        "# Impute zeros for reviews_per_month null values\n",
        "data['reviews_per_month'] = data['reviews_per_month'].fillna(0)\n",
        "\n",
        "# Impute empty string for name and host_name null values\n",
        "data['name'] = data['name'].fillna('')\n",
        "data['host_name'] = data['host_name'].fillna('')\n",
        "\n",
        "data.head(3)"
      ],
      "id": "30397a80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Splitting Data\n"
      ],
      "id": "74b611c5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_df, test_df = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# numeric data\n",
        "numerical_data = data.select_dtypes(include=[\"int64\", \"float64\"])\n",
        "numerical_data = numerical_data.columns\n",
        "\n",
        "# text data\n",
        "text_data = [\"name\"]\n",
        "\n",
        "# drop data\n",
        "drop_data = [\"host_name\", \"host_id\", \"id\"]\n",
        "\n",
        "# Categorical Data\n",
        "categorical_data = data.select_dtypes(include=[\"object\"])\n",
        "categorical_data = categorical_data.columns\n",
        "columns_to_exclude = text_data + drop_data\n",
        "categorical_data = [col for col in categorical_data if col not in columns_to_exclude]\n",
        "\n",
        "\n",
        "print(f'Train Shape: {train_df.shape}\\nTest Shape: {test_df.shape}\\n')\n",
        "print(f\"Numerical Columns: {numerical_data}/n\")\n",
        "print(f\"Categorical Columns: {categorical_data}\\n\")\n",
        "print(f\"Text Data: {text_data}\\n\")\n",
        "print(f\"Drop Columns: {drop_data}\\n\")"
      ],
      "id": "c99db168",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}